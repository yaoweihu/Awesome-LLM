# Awesome-LLM
Summary of LLM Resources.

## Content

- [1. Survey](#1-survey)
- [2. Model](#2-model)
- [3. Pre-training](#3-pre-training)
- [3. Finetune](#4-finetune)
- [5. Inference](#5-inference)
- [6. Small LLM](#small-llm)
- [7. Miscellaneous](#6-miscellaneous)
  
## 1. Survey
- [Small Language Models: Survey, Measurements, and Insights](https://arxiv.org/abs/2409.15790)   
  2024.09 - Zhenyan Lu - Beijing University of Posts and Telecommunications  
- [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196)  
  2024.02 - Shervin Minaee - Snap Inc., USA  
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)  
  2023.11 - Wayne Xin Zhao - Renmin Runiversity, China

## 2. Model
#### DeepSeek Series - DeepSeek-AI, China
- [DeepSeek-V3 Technical Report](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) - 2024.12  
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434) - 2024.05  
- [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954) - 2024.01
#### LLaMA Series - Meta Inc, USA
- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) - 2024.07  
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) - 2023.07  
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) - 2023.02
#### Mamba Series
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)  
  2023.12 - Albert Gu, Tri Dao - CMU, Princeton University  
- [Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers](https://arxiv.org/abs/2110.13985)  
  2021.10 - Albert Gu - Stanford University    

#### FlashAttention Series
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)  
  2022.05 - Tri Dao - Stanford University  

## 3. Pre-training
- [Teaching Transformers Causal Reasoning through Axiomatic Training](https://arxiv.org/abs/2407.07612)âœ…    
  2024.07 - Aniket Vashishtha - Microsoft Research, India  

## 4. Finetune
- [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608)  
  2024.07 - Zeyu Han - Northeastern University, USA

## 5. Inference
- [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/abs/2404.14294)  
  2024.07 - Zixuan Zhou - Tsinghua University, China  
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)  
  2023.09 - Woosuk Kwon - UC Berkeley  
- [Clipper: A Low-Latency Online Prediction Serving System](https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw)  
  2017.03 - Daniel Crankshaw - UC Berkeley, USA  

## 6. Small LLM
- [Small Models are Valuable Plug-ins for Large Language Models](https://arxiv.org/abs/2305.08848)  
  2024.05 - Canwen Xu - University of California, San Diego  

## 7. Miscellaneous
- [LiL'Log](https://lilianweng.github.io/archives/)
- [AI-System](https://learning-systems.notion.site/AI-Systems-LLM-Edition-294-162-Fall-2023-661887583bd340fa851e6a8da8e29abb)
- [Deep Generative Models](https://mit-6s978.github.io/schedule.html)  
